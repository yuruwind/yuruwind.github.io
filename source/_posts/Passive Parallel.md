---
title: Programming Massively Parallel Processors Fourth Edition 学习摘录
date: 2026-01-13 22:46:12
category:
  - Notes
tags: [并行计算]
math: true
---

> 本文主体摘抄于[李理的博客](https://fancyerii.github.io/)，在其基础上选择了最核心最重要的部分，基本上是这本书的精简版，可用作复习和回顾使用

# 第一章：简介

## 1.1 异构并行计算

低延迟的算术单元、复杂的操作数传递逻辑、大缓存内存和控制逻辑消耗了本可以用于提供更多算术执行单元和内存访问通道的芯片面积和功耗。这种设计方法通常被称为面向延迟设计。

**图1.1 CPU和GPU具有根本不同的设计理念：（A）CPU设计是面向延迟的；（B）GPU设计是面向吞吐量的。**



一个重要的观察是，在功耗和芯片面积方面，减少延迟比增加吞吐量昂贵得多。例如，可以通过将算术单元数量翻倍来使算术吞吐量翻倍，代价是将芯片面积和功耗翻倍。然而，将算术延迟减半可能需要将电流翻倍，代价是使用的芯片面积增加超过两倍，并将功耗增加四倍。**因此，GPU中的主流解决方案是优化大量线程的执行吞吐量，而不是减少单个线程的延迟**。这种设计方法通过允许流水线式内存通道和算术操作具有长延迟来节省芯片面积和功耗。内存访问硬件和算术单元的面积和功耗的降低允许GPU设计者在芯片上拥有更多这些组件，从而增加总执行吞吐量。

图1.1通过图示在图1.1A的CPU设计中较少数量的较大算术单元和较少数量的内存通道，与图1.1B中较多数量的较小算术单元和较多数量的内存通道之间的设计方法差异，直观地说明了这种差异。



是的，现代高性能CPU的核心普遍采用乱序执行（Out-of-Order Execution, OoOE）技术。

正如您在问题中提到的英特尔多核服务器处理器，其每个核心都是“乱序、多指令执行处理器”。这并非特例，而是行业标准。乱序执行是现代处理器用来提升性能的一项关键技术，它允许CPU动态地重新排列指令的执行顺序，只要不违反程序的数据依赖关系，从而更充分地利用处理器内部的执行单元2。

这项技术被广泛应用于主流的高性能处理器架构中，包括Intel和AMD的x86/x86-64处理器，以及许多高性能的ARM处理器（如您提到的Ampere处理器）。可以说，只要是追求高性能的现代CPU核心，几乎都具备乱序执行能力4。



图1.1B中的小缓存存储器旨在帮助控制这些应用程序的带宽要求，以便访问相同内存数据的多个线程不必全部访问DRAM

- 很多并行线程往往会**重复访问相同的数据**（比如同一个像素值、同一个权重参数）。
- 如果每个线程都直接去访问主存（DRAM），会造成巨大的**内存带宽压力**，而且速度慢。
- 因此，GPU配备了**小容量但高速的缓存**（如共享内存、L1/L2缓存），把热点数据暂存起来。
- 多个线程可以**共享缓存中的同一份数据**，避免反复访问慢速的DRAM，从而节省带宽、提升效率。



当程序具有大量线程时，具有更高执行吞吐量的GPU可以实现比CPU高得多的性能。因此，人们应该期望许多应用程序同时使用CPU和GPU，将顺序部分在CPU上执行，将数值密集型部分在GPU上执行。这就是为什么NVIDIA在2007年推出的Compute Unified Device Architecture（CUDA）编程模型被设计为支持应用程序的联合CPU-GPU执行的原因



## 1.2 为什么需要更高速度或并行性？

通过大幅提高计算吞吐量实现的新应用的一个重要例子是基于人工神经网络的深度学习。尽管自上世纪70年代以来神经网络一直在积极研究，但它们在实际应用中效果不佳，因为训练这些网络需要太多标记数据和太多计算资源。互联网的兴起提供了大量标记图片，而GPU计算吞吐量的提升则带来了大量计算资源。因此，自2012年以来，基于神经网络的应用在计算机视觉和自然语言处理领域迅速得到采用。这种采用已经彻底改变了计算机视觉和自然语言处理应用，并促使了自动驾驶汽车和家庭助手设备的快速发展。



## 1.3 加速实际应用程序

**并行计算系统相对于串行计算系统可以实现的加速度取决于可以并行化的应用程序部分。**例如，如果在可以并行化的部分花费的时间百分比为 30％，那么并行部分的 100倍 速度提升将最多减少应用程序的总执行时间 29.7%。也就是说，整个应用程序的加速度只约为 1/(1-0.297)=1.423。事实上，即使在可以并行化的部分可以实现无限的速度提升，也只能在执行时间上削减 30%，最多达到 1.433 倍的速度提升。**通过并行执行可以实现的速度提升水平可能受到应用程序可并行化部分的严重限制，这被称为阿姆达尔定律（Amdahl, 2013）。**



影响应用程序可实现加速度水平的另一个重要因素是从内存访问数据的速度以及向内存写入数据的速度。在实践中，应用程序的直接并行化通常会饱和内存（DRAM）带宽，导致只有约 10倍 的速度提升



## 1.4 并行编程中的挑战

幸运的是，大多数这些挑战已经得到研究人员的解决。在不同应用领域之间存在共同的模式，允许我们将在一个领域中推导出的解决方案应用到其他领域的挑战中。这是为什么我们将在重要的并行计算模式和应用程序的上下文中呈现解决这些挑战的关键技术的主要原因。



## 1.5 相关的并行编程接口

在过去的几十年中，提出了许多并行编程语言和模型（Mattson等，2004）。**最广泛使用的是用于共享内存多处理器系统的OpenMP（Open, 2005）和用于可伸缩集群计算的消息传递接口（MPI）（MPI, 2009）**。两者都已成为主要计算机供应商支持的标准化编程接口。

OpenMP实现包括编译器和运行时。程序员通过指定关于循环的指令（commands）和编译器的提示（hints）向OpenMP编译器提供信息。使用这些指令和提示，OpenMP编译器生成并行代码。运行时系统通过管理并行线程和资源来支持并行代码的执行。OpenMP最初是为CPU执行而设计的，并已扩展以支持GPU执行。	这种自动化和抽象有助于使应用代码在由不同供应商生产的系统以及同一供应商的不同系统世代之间更具可移植性。我们将这种属性称为性能可移植性

根据我们的经验，OpenMP编译器仍在不断发展和改进。许多程序员可能需要在OpenMP编译器存在不足的部分使用CUDA风格的接口。



  另一方面，MPI是一个计算节点在集群中不共享内存的编程接口（MPI, 2009）。所有数据共享和交互都必须通过显式消息传递来完成。MPI在高性能计算（HPC）中被广泛使用。在MPI中编写的应用程序已经成功在具有超过100,000个节点的集群计算系统上运行。今天，许多HPC集群使用异构的CPU/GPU节点。将应用程序移植到MPI中所需的工作量可能会相当大，这是由于计算节点之间缺乏共享内存。程序员需要进行领域分解，将输入和输出数据分区到各个节点。基于领域分解，程序员还需要调用消息发送和接收函数来管理节点之间的数据交换。相比之下，CUDA为GPU中的并行执行提供了共享内存以解决这一困难。虽然CUDA是与每个节点有效通信的接口，但大多数应用程序开发人员需要使用MPI在集群级别进行编程。此外，通过诸如NVIDIA Collective Communications Library（NCCL）的API，CUDA对多GPU编程的支持也越来越多。因此，对于在现代计算集群中使用多GPU节点的并行程序员来说，理解如何进行MPI/CUDA联合编程是非常重要的，这是在第20章“编程异构计算集群”中介绍的一个主题。



在2009年，包括苹果、英特尔、AMD/ATI和NVIDIA在内的几家主要行业参与者共同开发了一种标准化的编程模型，称为Open Compute Language（OpenCL）（The Khronos Group, 2009）。与CUDA类似，OpenCL编程模型定义了语言扩展和运行时API，以允许程序员管理大规模并行处理器中的并行性和数据传递。与CUDA相比，OpenCL更多地依赖于API，而不是语言扩展。这使得供应商可以快速调整其现有的编译器和工具以处理OpenCL程序。OpenCL是一个标准化的编程模型，使用OpenCL语言扩展和API支持的所有处理器上的应用程序可以在不修改的情况下正确运行。但是，为了在新处理器上实现高性能，可能需要修改应用程序。

熟悉OpenCL和CUDA的人会知道，在OpenCL和CUDA的关键概念和特性之间存在显着的相似性。也就是说，CUDA程序员可以在很小的努力下学习OpenCL编程。更重要的是，几乎在CUDA中学到的所有技术都可以轻松应用于OpenCL编程。



## 1.6 总体目标

## 1.7 书籍组织结构

https://fancyerii.github.io/pmpp/ch1/#12-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E6%9B%B4%E9%AB%98%E9%80%9F%E5%BA%A6%E6%88%96%E5%B9%B6%E8%A1%8C%E6%80%A7





# 第二章：异构数据并行计算

为了将彩色图像（图2.1左侧）转换为灰度图像（右侧），我们通过应用以下加权和公式计算每个像素的亮度值L：



L=0.21r+0.72g+0.07b

**RGB色彩图像表示**

在RGB表示中，图像中的每个像素都以(r, g, b)值的元组形式存储。图像的行的格式为(r g b) (r g b) . . . (r g b)，如下概念图片所示。每个元组指定了红色（R）、绿色（G）和蓝色（B）的混合。也就是说，对于每个像素，r、g 和 b 的值表示在呈现像素时红色、绿色和蓝色光源的强度（0表示黑暗，1表示完全强度）。

这三种颜色的实际允许混合方式在行业指定的色彩空间中有所不同。



**任务并行性与数据并行性** 在并行编程中，数据并行性并不是唯一使用的并行性类型。任务并行性在并行编程中也被广泛使用。任务并行性通常通过对应用程序的任务分解来暴露。例如，一个简单的应用程序可能需要进行矢量加法和矩阵-矢量乘法。其中每个都将是一个任务。如果这两个任务可以独立完成，那么就存在任务并行性。I/O和数据传输也是任务的常见来源。在大型应用程序中，通常存在更多独立的任务，因此也存在更多的任务并行性。

总体而言，数据并行性是并行程序可伸缩性的主要来源。



## 2.2 CUDA C程序结构

顾名思义，CUDA C建立在NVIDIA的CUDA平台上。CUDA目前是最成熟的用于大规模并行计算的框架，被广泛应用于高性能计算行业，提供了在大多数常见操作系统上使用的编译器、调试器和性能分析工具等基本工具。

CUDA C程序的结构反映了计算机中主机（CPU）和一个或多个设备（GPU）的共存。每个CUDA C源文件可以包含主机代码和设备代码的混合。默认情况下，任何传统的C程序都是一个只包含主机代码的CUDA程序。可以将设备代码添加到任何源文件中。设备代码使用特殊的CUDA C关键字明确定义。设备代码包括函数或内核，其代码以数据并行方式执行。



CUDA程序的执行过程如图2.3所示。执行从主机代码（CPU串行代码）开始。当调用内核函数时，在设备上启动大量线程以执行内核。由内核调用启动的所有线程被集体称为一个网格。这些线程是CUDA平台中并行执行的主要工具。图2.3显示了两个线程网格的执行过程。我们将很快讨论这些网格是如何组织的。当一个网格的所有线程都完成执行时，该网格终止，并且执行继续在主机上，直到启动另一个网格。

![](https://fancyerii.github.io/img/pmpp/ch2/4.png)



​	请注意，图2.3显示了一个简化的模型，其中CPU执行和GPU执行不重叠。许多异构计算应用程序管理重叠的CPU和GPU执行，以充分利用CPU和GPU的优势。

启动一个网格通常会生成许多线程，以利用数据并行性。在将颜色转为灰度的示例中，每个线程可以用于计算输出数组O的一个像素。在这种情况下，由网格启动生成的线程数等于图像中的像素数。**对于大图像，将生成大量线程。CUDA程序员可以假设这些线程在生成和调度时需要很少的时钟周期，这归功于高效的硬件支持。这一假设与传统的CPU线程形成对比，后者通常需要数千个时钟周期来生成和调度。**



**线程** 线程是现代计算机中处理器执行顺序程序的简化视图。一个线程包括程序的代码、正在执行的代码点以及其变量和数据结构的值。就用户而言，线程的执行是顺序的。用户可以使用源代码级调试器逐条执行语句，查看下一条将要执行的语句，并在执行过程中检查变量和数据结构的值。

线程在编程中已经使用了很多年。如果程序员希望在应用程序中启动并行执行，他/她可以使用线程库或特殊语言创建和管理多个线程。在CUDA中，每个线程的执行也是顺序的。CUDA程序通过调用内核函数启动并行执行，这会导致底层运行时机制启动一个处理不同数据部分的线程网格。



> 🔸 所以 CUDA 文档说“每个线程的执行也是顺序的”，指的是**单个线程的语义顺序**，而不是说所有线程串行执行。

### 四、总结：三个层次要分清



| 层次                       | 描述                        | 是否“顺序”？                     |
| -------------------------- | --------------------------- | -------------------------------- |
| **1. 编程模型 / 语言语义** | 程序员写的代码逻辑          | ✅ 必须顺序（保证正确性）         |
| **2. 单线程硬件执行**      | CPU/GPU核心如何执行一个线程 | ⚠️ 内部可能乱序，但结果等效于顺序 |
| **3. 多线程并发/并行**     | 多个线程是否同时运行        | ❌ 并行（多个线程同时执行）       |





## 2.3 矢量加法内核

请注意，修改后的vecAdd函数本质上是一个外包代理，将输入数据发送到设备，激活设备上的计算，并从设备收集结果。该代理以一种使主程序甚至无需知道矢量加法实际上是在设备上完成的方式执行此操作。实际上，由于数据的来回复制，这种“透明”外包模型通常效率较低。通常，人们会在设备上保留大型和重要的数据结构，并仅从主机代码中调用设备函数。然而，目前我们将使用简化的透明模型来介绍基本的CUDA C程序结构。



## 2.4 设备全局内存和数据传输

- 对于矢量加法核函数，在调用核函数之前，程序员需要在设备全局内存中分配空间并将数据从主机内存传输到设备全局内存中的已分配空间。
- 同样，在设备执行后，程序员需要将结果数据从设备全局内存传输回主机内存，并释放在设备全局内存中分配的不再需要的空间。
- CUDA运行时系统（通常在主机上运行）提供了应用程序编程接口（API）函数，代表程序员执行这些活动。从这一点开始，我们将简单地说数据从主机传输到设备，以简称将数据从主机内存复制到设备全局内存中。相同的情况适用于相反的方向。



**cudaMalloc函数的第一个参数是一个指针变量的==地址==(注意是 &A_d ! 而不是A_d)，该变量将被设置为指向已分配对象的地址。**指针变量的地址应强制转换为(void *)，因为该函数期望一个通用指针；内存分配函数是一个通用函数，不限于任何特定类型的对象。这个参数允许cudaMalloc函数将分配的内存的地址写入提供的指针变量，而不管其类型如何。调用核函数的主机代码将此指针值传递给需要访问已分配内存对象的核函数。cudaMalloc函数的**第二个参数给出要分配的数据的大小，以字节为单位。**该第二个参数的使用与C malloc函数的size参数一致。

**注**:

- CUDA C还具有更先进的库函数，用于在主机内存中分配空间。我们将在第20章“编程异构计算集群”中讨论它们。
- 事实上，cudaMalloc返回一个通用对象，这使得使用动态分配的多维数组更加复杂。我们将在第3.2节解决这个问题。
- 请注意，cudaMalloc与C的malloc函数具有不同的格式。C的malloc函数返回指向分配对象的指针。它只需要一个参数，指定分配对象的大小。而cudaMalloc函数写入作为第一个参数给出的指针变量的地址。因此，cudaMalloc函数需要两个参数。cudaMalloc的这种两参数格式使其能够使用返回值以与其他CUDA API函数相同的方式报告任何错误。



```cpp
现在，我们使用以下简单的代码示例来说明cudaMalloc和cudaFree的用法：

#include <cuda_runtime.h> 
int main() {
    // Declare device pointers
    float *A_d, *B_d, *C_d;

    // Allocate space in the device global memory for A
    cudaMalloc((void**)&A_d, size);

    // Your computation with A_d goes here

    // Free the allocated memory for A
    cudaFree(A_d);

    return 0;
}
```



一旦主机代码为数据对象在设备全局内存中分配了空间，它可以请求将数据从主机传输到设备。这通过调用CUDA API函数之一来完成。图2.7展示了这样一个API函数，cudaMemcpy。

cudaMemcpy函数有四个参数。

- 第一个参数是指向要复制的数据对象目标位置的指针。(Pointer to destination)

- 第二个参数指向源位置。

- 第三个参数指定要复制的字节数。

- 第四个参数指示复制涉及的内存类型：从主机到主机，从主机到设备，从设备到主机以及从设备到设备。

  例如，内存复制函数可用于将数据从设备全局内存中的一个位置复制到设备全局内存中的另一个位置。



vecAdd函数调用cudaMemcpy函数将A_h和B_h向量从主机内存复制到A_d和B_d在设备内存中，然后将它们相加，并在完成相加后将C_d向量从设备内存复制到C_h在主机内存中。

假设A_h、B_h、A_d、B_d和size的值已经设置好，下面是三个cudaMemcpy调用的示例。cudaMemcpyHostToDevice和cudaMemcpyDeviceToHost**是CUDA编程环境中的已识别的预定义常量**。请注意，通过正确排序源和目标指针并使用适当的常量进行传输类型，可以使用相同的函数在两个方向上传输数据。

```
cudaMemcpy(A_d, A_h, size, cudaMemcpyHostToDevice);
cudaMemcpy(B_d, B_h, size, cudaMemcpyHostToDevice);
// ...
cudaMemcpy(C_h, C_d, size, cudaMemcpyDeviceToHost);
```



总结一下，在图2.4中，主程序调用vecAdd，该函数也在主机上执行。vecAdd函数（见图2.5的概要）在设备全局内存中分配空间，请求数据传输，并调用执行实际矢量加法的核函数。我们将这种主机代码称为**用于调用核函数的存根(stub)**。





## **CUDA中的错误检查和处理** 

通常，对于程序来说，检查并处理错误是很重要的。 CUDA API函数在提供服务时返回指示错误是否发生的标志。大多数错误是由于调用中使用了不适当的参数值。 为简洁起见，我们在示例中不会显示错误检查代码。例如，图2.9中显示了对cudaMalloc的调用：

```
cudaMalloc((void**) &A_d, size);
```

实际上，我们应该将调用包围在测试错误条件的代码中，并打印出错误消息，以便用户能够意识到发生了错误。这样的检查代码的简单版本如下：

```
cudaError_t err = cudaMalloc((void**) &A_d, size);
if (err != cudaSuccess) {
    printf("%s in %s at line %d\n",
        cudaGetErrorString(err),
        __FILE__, __LINE__);
    exit(EXIT_FAILURE);
}
```

这样，如果系统没有设备内存，用户将得到关于这种情况的通知。这可以节省许多调试时间。可以定义一个C宏，使源代码中的检查代码更简洁。





## 2.5 核函数和线程

当程序的主机代码调用核函数时，CUDA运行时系统会启动一个线程网格，该网格组织成两级层次结构。每个网格都组织成一个线程块数组，我们将其简称为块。一个网格中的所有块都是相同大小的；**每个块在当前系统上最多可以包含1024个线程**。



**内建变量**

 许多编程语言都有内建变量。这些变量具有特殊的含义和目的。这些变量的值通常由运行时系统预初始化，并在程序中通常是只读的。程序员应该避免重新定义这些变量以供其他用途使用。

每个线程块中的总线程数是在调用核函数时由主机代码指定的。同一核函数可以在主机代码的不同部分以不同数量的线程调用。对于给定的线程网格，**块中的线程数可以在名为blockDim的内建变量中找到。blockDim变量是一个结构，包含三个无符号整数字段（x、y和z），这些字段帮助程序员将线程组织成一维、二维或三维数组。对于一维组织，仅使用x字段。**对于二维组织，使用x和y字段。对于三维结构，使用所有三个x、y和z字段。线程的组织方式通常反映数据的维度。这是有道理的，因为线程是为了并行处理数据而创建的，因此线程的组织方式自然应该反映数据的组织方式。

在图2.9中，每个线程块被组织成一个一维数组的形式，因为数据是一维向量。blockDim.x变量的值表示每个块中的总线程数，在图2.9中为256。**通常建议线程块的每个维度的线程数是32的倍数，出于硬件效率的原因。**稍后我们将重新讨论这一点。

![](https://fancyerii.github.io/img/pmpp/ch2/10.png)



**CUDA核函数可以访问另外两个内建变量（threadIdx和blockIdx），这些变量允许线程彼此区分，并确定每个线程要处理的数据区域。**threadIdx变量为每个线程提供块内的唯一坐标。在图2.9中，由于我们使用的是一维线程组织，仅使用threadIdx.x。图2.9中每个线程的threadIdx.x值显示在每个线程的小阴影框中。每个块中的第一个线程的threadIdx.x变量的值为0，第二个线程的值为1，第三个线程的值为2，依此类推。

**分层组织** 与CUDA线程一样，许多实际系统都是分层组织的。美国电话系统就是一个很好的例子。在顶层，电话系统由“区域”组成，每个区域对应一个地理区域。同一区域内的所有电话线都具有相同的3位“区号”。电话区域有时比城市大。例如，伊利诺伊州中部的许多县和城市都属于同一电话区域，并共享相同的区号217。在一个区域内，每条电话线都有一个七位数的本地电话号码，这使得每个区域最多可以拥有约一千万个号码。 可以将每条电话线视为一个CUDA线程，其中区号是blockIdx的值，而七位本地号码是threadIdx的值。这种分层组织允许系统拥有大量的电话线，同时保留对同一区域进行呼叫的“局部性”。也就是说，在拨打同一区域内的电话时，呼叫者只需拨打本地号码。只要我们大多数时间都在本地区域内拨打电话，我们很少需要拨打区号。如果我们偶尔需要拨打另一个区域的电话，我们拨打1和区号，然后是本地号码。 （这就是为什么任何区域的本地号码都不应以1开头的原因。）CUDA线程的分层组织也提供了一种局部性形式。我们将很快研究这种局部性。



在图2.9中，计算了一个唯一的全局索引i，即i=blockIdx.x * blockDim.x + threadIdx.x。回顾一下，我们的示例中blockDim的值为256。块0中线程的i值范围从0到255。块1中线程的i值范围从256到511。块2中线程的i值范围从512到767。也就是说，这三个块中线程的i值形成了从0到767的连续覆盖。由于每个线程使用i来访问A、B和C，这些线程涵盖了原始循环的前768次迭代。通过启动具有更多块的网格，可以处理更大的向量。通过启动具有n个或更多线程的网格，可以处理长度为n的向量。

![](https://fancyerii.github.io/img/pmpp/ch2/11.png)



图2.10显示了一个进行向量相加的核函数。请注意，在核函数中我们不使用“_h”和“_d”约定，因为这里没有潜在的混淆。在我们的示例中，核的语法是ANSI C，并带有一些显著的扩展。首先，在vecAddKernel函数的声明前面有一个CUDA-C特定的关键字“global”。此关键字表示该函数是一个核函数，可以调用它在设备上生成一个线程网格。

![](https://fancyerii.github.io/img/pmpp/ch2/12.png)

通常，CUDA C使用三个修饰关键字扩展了C语言，这些关键字可以在函数声明中使用。这些关键字的含义总结如图2.11所示。“__global__”关键字表示被声明的函数是一个CUDA C核函数。请注意，“__global__”一词两侧都有两个下划线字符。这样的核函数在设备上执行，并且可以从主机调用。在支持动态并行性的CUDA系统中，它也可以从设备调用，我们将在第21章“CUDA动态并行性”中看到。**重要的特点是调用这样一个核函数会在设备上启动一个新的线程网格。**

“__device__”关键字表示被声明的函数是CUDA设备函数。设备函数在CUDA设备上执行，只能从核函数或另一个设备函数调用。**设备函数由调用它的设备线程执行，不会导致启动任何新的设备线程。**7

“__host__”关键字表示被声明的函数是CUDA主机函数。主机函数只是在主机上执行的传统C函数，只能从另一个主机函数调用。**默认情况下，如果在其声明中没有任何CUDA关键字，则CUDA程序中的所有函数都是主机函数**。这是有道理的，因为许多CUDA应用程序是从仅CPU执行环境迁移过来的。在迁移过程中，程序员会在主机函数中添加核函数和设备函数。原始函数仍然保留为主机函数。将所有函数默认为主机函数可以免去程序员修改所有原始函数声明的繁琐工作。



请注意，可以在函数声明中同时使用“__host__”和“__device__”。这种组合告诉编译系统**为同一函数生成两个版本的目标代码**。其中一个在主机上执行，只能从主机函数调用。另一个在设备上执行，只能从设备或核函数调用。这支持常见的用例，即相同的函数源代码可以重新编译以生成设备版本。许多用户库函数很可能属于这个类别。



**图2.10中有一个自动（局部）变量i。在CUDA核函数中，自动变量对于每个线程都是私有的。**也就是说，每个线程都会生成一个i的版本。如果使用10,000个线程启动网格，将会有10,000个版本的i，每个线程一个版本。由线程分配给其i变量的值对其他线程不可见。我们将在第5章“内存架构和数据局部性”中更详细地讨论这些自动变量。

通过将图2.4和图2.10进行快速比较，可以对CUDA核函数有一个重要的了解。图2.10中的核函数没有对应于图2.4中的循环。读者应该问循环去哪了。**答案是循环现在被线程网格替代了**。整个网格形成了循环的等效部分。网格中的每个线程对应于原始循环的一次迭代。这有时被称为**循环并行性，其中原始顺序代码的迭代由线程并行执行。**



请注意，图2.10中的addVecKernel函数有一个if (i < n)语句。这是因为并非所有的矢量长度都可以表示为块大小的倍数。例如，假设矢量长度是100。最小的有效线程块维度是32。假设我们选择32作为块大小。将需要启动四个线程块来处理所有100个矢量元素。然而，这四个线程块**将有128个线程。我们需要禁用第3个线程块中的最后28个线程，以防它们执行原始程序不期望的工作。**由于所有线程都将对它们的i值进行与n的比较，因此所有线程将测试它们的i值是否小于n，其中n是100。**通过if (i < n)语句**，前100个线程将执行加法，而最后的28个线程将不执行。这允许调用该核函数来处理任意长度的矢量。



- 7 我们将在稍后解释在不同CUDA生成中使用间接函数调用和递归的规则。总的来说，**为了实现最大的可移植性，应该避免在设备函数和核函数中使用递归和间接函数调用。**



## 2.6 调用核函数

完成了核函数的实现后，剩下的步骤是从主机代码中调用该函数以启动网格。这在图2.12中进行了说明。当主机代码调用核函数时，它通过执行配置参数设置网格和线程块的维度。配置参数位于传统C函数参数之前的“<<<”和“>>>”之间。**第一个配置参数给出了网格中的块数，第二个指定了每个块中的线程数。**

在这个例子中，每个块中有256个线程。**为了确保我们有足够的线程在网格中覆盖所有的向量元素**，我们需要将网格中的块数设置为所需线程数（在这种情况下为n）除以线程块大小（在这种情况下为256）的上取整（将商四舍五入为较高的整数值）。有许多执行上取整的方法。**一种方法是对n/256.0应用C天花板函数**。使用浮点值256.0确保我们生成一个浮点值，以便天花板函数可以正确地将其上取整。

![](https://fancyerii.github.io/img/pmpp/ch2/14.png)

请注意，**所有线程块都在向量的不同部分上操作。它们可以以任意顺序执行。程序员不能对执行顺序做出任何假设**。具有较少执行资源的小型GPU可能仅以并行方式执行一个或两个这些线程块。较大的GPU可能并行执行64或128个块。这使得CUDA核函数具有硬件执行速度的可伸缩性。也就是说，相同的代码在小型GPU上以较低的速度运行，在大型GPU上以较高的速度运行。我们将在第4章《计算架构和调度》中重新讨论这一点。



## 2.7 编译

代码需要由一个能够识别和理解这些扩展的编译器编译，比如NVCC（NVIDIA C编译器）。

![](https://fancyerii.github.io/img/pmpp/ch2/15.png)



## 2.8 总结

本章提供了CUDA C编程模型的快速、简化概述。CUDA C扩展了C语言以支持并行计算。我们在本章中讨论了这些扩展的基本子集。为方便起见，我们总结了本章中讨论的扩展如下：

### 2.8.1 函数声明

CUDA C扩展了C函数声明语法，以支持异构并行计算。这些扩展总结在图2.12中。使用“__global__”、“__device__”或“__host__”中的一个，CUDA C程序员可以指示编译器生成内核函数、设备函数或主机函数。所有没有这些关键字的函数声明默认为主机函数。如果在函数声明中同时使用“__host__”和“__device__”，编译器将为设备和主机分别生成两个版本的函数。如果函数声明没有任何CUDA C扩展关键字，该函数默认为主机函数。

### 2.8.2 内核调用和网格启动

CUDA C扩展了C函数调用语法，使用由“<<<”和“>>>”括起的内核执行配置参数。这些执行配置参数仅在调用内核函数以启动网格时使用。我们讨论了定义网格维度和每个块维度的执行配置参数。读者应参阅CUDA编程指南（NVIDIA，2021）以获取有关内核启动扩展以及其他类型执行配置参数的更多详细信息。

### 2.8.3 内建（预定义）变量

CUDA内核可以访问一组内建的、预定义的只读变量，允许每个线程与其他线程区分开，并确定要处理的数据区域。在本章中，我们讨论了threadIdx、blockDim和blockIdx变量。在第3章“多维网格和数据”中，我们将详细讨论使用这些变量的更多细节。

### 2.8.4 运行时应用程序编程接口

CUDA支持一组API函数，为CUDA C程序提供服务。我们在本章中讨论的服务是cudaMalloc、cudaFree和cudaMemcpy函数。这些函数由主机代码调用，以代表调用程序分配设备全局内存、释放设备全局内存和在调用程序的代表上在主机和设备之间传输数据。读者请参阅CUDA C编程指南，了解其他CUDA API函数。 我们本章的目标是介绍CUDA C的核心概念以及对C的基本扩展，以编写一个简单的CUDA C程序。该章节绝不是所有CUDA功能的全面介绍。这些功能的一些将在本书的其余部分中进行介绍。然而，我们的重点将放在这些功能支持的关键并行计算概念上。我们将只介绍我们代码示例所需的CUDA C功能，用于并行编程技术。总的来说，我们鼓励读者随时查阅CUDA C编程指南，以获取有关CUDA C功能的更多详细信息。





# 第三章：多维网格和数据

## 3.1 多维网格组织

请注意，dimBlock 和 dimGrid 是由程序员定义的主机代码变量。只要它们具有 dim3 类型，它们就可以具有任何合法的 C 变量名。例如，以下语句实现了与上述语句相同的结果：

网格和块的维度也可以从其他变量计算。例如，图2.12中的内核调用可以写成如下形式：

![](https://fancyerii.github.io/img/pmpp/ch3/3.png)



为了方便起见，CUDA 提供了一种特殊的快捷方式，用于调用具有一维（1D）网格和块的内核

熟悉 C++ 的读者会意识到，这种**用于 1D 配置的“简写”方便的实现是通过 C++ 构造函数和默认参数的工作方式来实现的**。dim3 构造函数的参数的默认值为 1。当在期望 dim3 的地方传递一个单一值时，该值将传递给构造函数的第一个参数，而第二个和第三个参数将采用默认值 1。结果是一个 1D 网格或块，其中 x 维度的大小是传递的值，y 和 z 维度的大小为 1。



在内核函数中，gridDim 和 blockDim 变量的 x 字段根据执行配置参数的值进行了预初始化。例如，如果 n 等于 4000，在 vectAddKernel 内核中对 gridDim.x 和 blockDim.x 的引用将分别得到 16 和 256。请注意，在内核函数中，与主机代码中的 dim3 变量不同，这**些变量的名称是 CUDA C 规范的一部分，不能更改。也就是说，gridDim 和 blockDim 是内核中的内置变量，**并始终反映网格和块的维度。



- 在 CUDA C 中，**gridDim.x** 允许的值范围从 1 到 **2^31−1**(设备的计算能力低于 3.0 的允许 blockIdx.x 的值在 1 到 216−1 之间 )，**gridDim.y 和 gridDim.z** 的值范围从 1 到 216−1（65,535）。

- **在当前的 CUDA 系统中，块的总大小限制为 1024 个线程。**这些线程可以以任何方式分布在三个维度上，只要总线程数不超过 1024。例如，(512, 1, 1)、(8, 16, 4) 和 (32, 16, 2) 这样的 blockDim 值都是允许的，但 (32, 32, 2) 不允许，因为总线程数将超过 1024。
- 网格及其块不需要具有相同的维度。网格的维度可以高于其块，反之亦然。



## 3.2 将线程映射到多维数据

有至少两种将2D数组线性化的方式。一种方法是将同一行的所有元素放入连续的位置。然后，这些行依次放入内存空间。这种排列称为行主排列

另一种线性化2D数组的方法是将同一列的所有元素放入连续的位置。然后，这些列依次放入内存空间。这种排列称为列主排列，被FORTRAN编译器使用。此外，许多设计用于FORTRAN程序的C库使用列主排列以匹配FORTRAN编译器的排列。因此，这些库的手册通常告诉用户如果从C程序调用这些库，他们应该对输入数组进行转置。

![](https://fancyerii.github.io/img/pmpp/ch3/10.png)

*图3.4 colorToGrayscaleConversion的源代码，具有2D线程映射到数据的功能。*